# Testing Philosophy Study Guide

## Intro to Testing

### Tester vs Developer Mindset
Testers are problem-oriented, concentrating on identifying issues and potential failures in software and the surrounding resources. Their goal is to ensure the application works under various conditions, often thinking about how to break the system to uncover weaknesses and vulnerabilities. They adopt the userâ€™s perspective to test for real-world scenarios and usability, aiming to ensure the software meets quality standards and is free of defects. Their focus is on quality assurance, making sure the final product is reliable and intuitive

Developers are primarily goal-oriented, focusing on building and implementing features that are functional, efficient, and scalable. They approach problems with a solution-focused mindset, always thinking about how to make things work. Attention to detail is crucial for them, as they strive to write clean, efficient code that performs well. They also assess risks in terms of technical challenges, scalability, and performance, ensuring that the software can handle its use demands

Fundamentally, the end goal for both testers and developers is the same: releasing a high quality product that meets the needs of stakeholders. They simply differ in their approach to assisting in the creation of a high quality product

### Quality Management
Despite the terms testing and **quality assurance** (**QA**) often being used synonymously, they are actually distinct concepts. Testing is a form of **quality control** (**QC**) that is product-oriented and corrective, focused on identifying and fixing defects in the product to achieve high levels of product quality. **QA**, on the other hand, is process-oriented and preventive, aiming to improve processes to ensure high quality results. This means that testing itself is a **QC** practice, whereas **QA** involves implementing and improving processes, ensuring that good processes lead to good products. Test results are used in **QC** to fix defects, and in **QA** to provide feedback on process performance

### Verification & Validation
Most testing can be categorized as either **verification** or **validation**. These two categories of tests have different objectives:
- **verification** answers the question "are we building the product correctly?"
- **validation** answers the question "are we building the right product?"

For example, imagine you are on a team that has been tasked with building and testing a bike. **Verification** would involve checking that each component of the bike meets the specified design requirements and standards. This could involve checking the frame is made from the correct material, the brakes are designed properly, the correct number of gears are accommodated in the plan, etc. Essentially, verification is about confirming that the bike is being assembled according to any provided requirements through the use of **static testing**. On the other hand, **validation** would involve testing the bike in real-world conditions to ensure it meets the needs and expectations of the end user. This might include taking the bike for a test ride to confirm it provides a comfortable and safe riding experience, and that it performs well under various conditions such as different terrains and weather. **Validation** ensures that the final product is fit for its intended purpose and satisfies customer requirements through the use of **dynamic testing**.

### Requirements
**Requirements** can vary widely, encompassing business, technical, regulatory, accessibility, and other arbitrary needs. They provide details about what should be included or excluded from a product. These **requirements** can take many forms, although there are some standardized methods for presenting them. Common formats include **software requirement specifications**, **use cases**, **user stories**, and **non-functional specifications**. Each of these formats helps ensure that all necessary aspects of the product are clearly defined and understood by developers, testers, and any other stakeholders

### Defect vs Error vs Failure
A core goal of testing is to discover **defects**: a **defect** is any failed requirement or deviation of an expected outcome. This could be a business **requirement** not being met, a software feature not working correctly, gaining access to resources you are not supposed to, etc. Note that the terms **defect** and "bug" can be used interchangeably, but **defect** is the more formal way of describing deviations from expected results

In software work, any mistake a developer adds to code, whether calling the wrong variable, forgetting to implement a business rule, or any other mistake, is called an **error**. It is **errors** that lead to **defects** being present in code

Anytime a **defect** is executed/triggered in your application it will trigger one or more **failures**. Note that **defects** and **failures** represent the same thing, a deviation from the expected or required. The main difference is in how the **defect** is revealed: **failures** happen when a **defect** is triggered in a active application (this can be user discovered in production or tester discovered in the testing environment) that causes the intended use of the application to deviate from the expected. This means that all **failures** are **defects**, but not all **defects** are **failures**. Some examples of **defects** that are not **failures**:
- aesthetic issues
- sub-par application performance (poor latency, throughput, etc.)
- incorrect documentation
- accessibility optimizations

### Automated Testing
**Automated testing** offers several advantages over **manual testing**. It is **not subject to human error**, ensuring more consistent and accurate results. **Automated tests** are **quicker than manual testing**, providing faster feedback and allowing for more frequent testing cycles. Although there is an initial upfront investment in setting up **automated tests**, it **saves time in the long run by reducing the need for repetitive manual testing**. **Automated tests** are reliable and **can run 24/7**, making them ideal for continuous integration and delivery pipelines. They **can handle complex and large applications**, ensuring thorough testing coverage. **Automated testing** is particularly **well-suited for unit, integration, and system testing**. **Automated Testing** is also useful for **dynamic testing**: testing done on running software

### Manual Testing
Manual testing has its own set of advantages and considerations. While it is **subject to human error**, this can be mitigated with thorough training and attention to detail. Manual testing may be **slower than automated testing**, but it allows for a more nuanced and flexible approach, especially when dealing with smaller or simpler applications or features. It requires **less upfront investment and technical knowledge**, making it more accessible for teams with limited resources. Manual testing is particularly **well-suited for acceptance testing**, where the focus is on ensuring the software meets the end-user's requirements and expectations and assessing subjective criteria that can not easily be automated (or automated at all). **Manual Testing** is also useful for **static testing**: testing done when software is not running. This kind of testing involves reviewing documentation, code syntax, and other auxiliary resources to the code itself

## Testing Components

### Test Analysis
**Test analysis** is a phase in the software testing process where testers systematically examine the information available to identify what needs to be tested. This involves understanding the **requirements**, **design**, and potential **risks** associated with the software. The goal of **test analysis** is to ensure that all relevant aspects of the software are considered, and appropriate **test conditions** are identified to guide test creation

### Test Basis
The **test basis** serves as the foundation for **test analysis** and design. It is a comprehensive body of knowledge that includes **requirement specifications, design documents, risk analysis reports, and traceability reports**. These resources provide the necessary information to understand the software's expected behavior and performance. By leveraging the **test basis**, testers can ensure that their testing efforts are aligned with the software's requirements and design, leading to more effective and thorough testing

### Test Condition
**Test conditions** are specific aspects of the software that need to be evaluated to ensure it meets the required standards and performs as expected. These conditions are derived from the **test basis** and can be categorized into various types, such as **functional**, **performance**, **security**, and **usability** test conditions (this is a non-exhaustive list). For example, **functional test conditions** might involve verifying that the login functionality works correctly, while **performance test conditions** could focus on ensuring the application can handle a high number of concurrent users. By identifying and addressing these **test conditions**, testers can create detailed **test cases** that systematically verify each aspect of the software, ensuring comprehensive coverage and high-quality results

### Test Objective
**Test objectives**, also known as "test goals", are the end-reasons for conducting testing activities. These objectives typically include evaluating work products to ensure they **meet the required standards** and **identifying defects** to improve the overall quality of the software. **Ensuring coverage and compliance with specified requirements** is another critical objective, as it helps maintain consistency and adherence to standards. **Reducing risk** by identifying potential issues early in the development process is also a key goal. **Providing general information** about the software's performance and behavior helps stakeholders make informed decisions. Ultimately, **building confidence in the product or service** is a fundamental objective, as it assures stakeholders that the software is reliable and meets their expectations

### Test Object
In the context of software testing, the term **test object** or "test item" refers to the resource being evaluated. Whether you are testing an individual module or the complete system, the **test object** is the focal point of your testing activities. Clearly defining your **test objects** helps ensure that all testing efforts are appropriately directed and the scope of testing is well understood. This helps testers develop targeted **test cases** and strategies that address the specific functionalities and requirements of the resource under examination. This approach helps in systematically uncovering defects, verifying compliance with requirements, and ultimately ensuring the quality and reliability of the product

### Test Case
A **test case** is a detailed set of **preconditions**, **inputs**, **actions**, and **expected results** that are developed based on your **test conditions**. Essentially, a **test case** is a formal collection of all the resources and steps needed to perform actual testing. The purpose of a **test case** is to check whether the software behaves as expected under the defined conditions. When a test case is executed, the results are compared against the expected outcomes to determine if the test has "passed" or "failed." A test "passes" if the actual outcome matches the expected result, indicating that the software is functioning correctly. Conversely, a test "fails" if the actual outcome deviates from the expected result, revealing a **defect** in the software. In a somewhat ironic way, testers "succeed" in their job when the tests they build fail due to unexpected application behavior; better the testers discover the defect than end users. By systematically developing and executing **test cases**, testers can ensure comprehensive coverage of the software's functionality and identify any defects that need to be addressed, helping to create high levels of quality for the end product

### Test Data
When creating **test cases** you will often have specific data sets for one or more of your **test cases**: this collection of data is your **test data**. Oftentimes initial **test data** can be aggregated and sorted during the **test analysis** activity, and then later specified once **test conditions** are determined. For instance, if your **test object** is a registration feature and your **test objective** is to validate username constraints are enforced by the application, during your **test analysis** you would determine what the constraints should be (character length, character types, unique, minimum number of non alphabet characters required, etc.), and then when designing your tests you would provide specific values that meet (or don't, depending on the type of testing you are doing) those constraints

### Test Suite
Similar or related **test cases** can be organized into groupings called **test suites**. These groupings can be due to related **test objectives**, **test objects**, **test data**, etc.

### Positive Testing
**Positive testing**, also known as "happy path testing," involves designing **test cases** using **test data** that fully complies with the software's requirements. This valid data is then used as input for relevant actions, with the expectation that these actions will be successfully executed. In essence, the goal of a **positive test** is to confirm that when the software is provided with valid input, it performs as expected and completes the intended actions without any issues. This type of testing ensures that the software functions correctly under normal, expected conditions

### Negative Testing
**Negative testing**, also known as "sad path testing," involves designing **test cases** using **test data** that does not conform to the software's requirements. This invalid or unexpected data is used as input to test how the software handles errors and edge cases. The goal of **negative testing** is to ensure that the software can gracefully handle invalid input and unexpected conditions without crashing or producing incorrect results. By intentionally providing invalid data, testers can check that the software has appropriate error handling mechanisms in place and that it remains stable and secure even when faced with adverse conditions. This type of testing is crucial for identifying potential vulnerabilities and ensuring the robustness of the software